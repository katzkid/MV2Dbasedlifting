{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data list info\n",
    "\n",
    "We follow the format of `nuscenes` dataset: \n",
    "\n",
    "nuscenes_database/xxxxx.bin: point cloud data included in each 3D bounding box of the training dataset\n",
    "\n",
    "nuscenes_infos_train.pkl: training dataset, a dict contains two keys: metainfo and data_list. metainfo contains the basic information for the dataset itself, such as categories, dataset and info_version, while data_list is a list of dict, each dict (hereinafter referred to as info) contains all the detailed information of single sample as follows:\n",
    "\n",
    "`info[‘sample_idx’]`: The index of this sample in the whole dataset.\n",
    "\n",
    "info[‘token’]: Sample data token.\n",
    "\n",
    "info[‘timestamp’]: Timestamp of the sample data.\n",
    "\n",
    "info[‘ego2global’]: The transformation matrix from the ego vehicle to global coordinates. (4x4 list)\n",
    "\n",
    "info[‘lidar_points’]: A dict containing all the information related to the lidar points.\n",
    "\n",
    "- info[‘lidar_points’][‘lidar_path’]: The filename of the lidar point cloud data.\n",
    "\n",
    "- info[‘lidar_points’][‘num_pts_feats’]: The feature dimension of point.\n",
    "\n",
    "- info[‘lidar_points’][‘lidar2ego’]: The transformation matrix from this lidar sensor to ego vehicle. (4x4 list)\n",
    "\n",
    "info[‘lidar_sweeps’]: A list contains sweeps information (The intermediate lidar frames without annotations)\n",
    "\n",
    "- info[‘lidar_sweeps’][i][‘lidar_points’][‘data_path’]: The lidar data path of i-th sweep.\n",
    "    \n",
    "- info[‘lidar_sweeps’][i][‘lidar_points’][‘lidar2ego’]: The transformation matrix from this lidar sensor to ego vehicle. (4x4 list)\n",
    "    \n",
    "- info[‘lidar_sweeps’][i][‘lidar_points’][‘ego2global’]: The transformation matrix from the ego vehicle to global coordinates. (4x4 list)\n",
    "    \n",
    "- info[‘lidar_sweeps’][i][‘lidar2sensor’]: The transformation matrix from the main lidar sensor to the current sensor (for collecting the sweep data). (4x4 list)\n",
    "    \n",
    "- info[‘lidar_sweeps’][i][‘timestamp’]: Timestamp of the sweep data.\n",
    "    \n",
    "- info[‘lidar_sweeps’][i][‘sample_data_token’]: The sweep sample data token.\n",
    "\n",
    "info[‘images’]: A dict contains six keys corresponding to each camera: 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_FRONT_LEFT', 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT'. Each dict contains all data information related to corresponding camera.\n",
    "\n",
    "- info[‘images’][‘CAM_XXX’][‘img_path’]: The filename of the image.\n",
    "    \n",
    "- info[‘images’][‘CAM_XXX’][‘cam2img’]: The transformation matrix recording the intrinsic parameters when projecting 3D points to each image plane. (3x3 list)\n",
    "    \n",
    "- info[‘images’][‘CAM_XXX’][‘sample_data_token’]: Sample data token of image.\n",
    "    \n",
    "- info[‘images’][‘CAM_XXX’][‘timestamp’]: Timestamp of the image.\n",
    "    \n",
    "- info[‘images’][‘CAM_XXX’][‘cam2ego’]: The transformation matrix from this camera sensor to ego vehicle. (4x4 list)\n",
    "    \n",
    "- info[‘images’][‘CAM_XXX’][‘lidar2cam’]: The transformation matrix from lidar sensor to this camera. (4x4 list)\n",
    "\n",
    "info[‘instances’]: It is a list of dict. Each dict contains all annotation information of single instance. For the i-th instance:\n",
    "\n",
    "- info[‘instances’][i][‘bbox_3d’]: List of 7 numbers representing the 3D bounding box of the instance, in (x, y, z, l, w, h, yaw) order.\n",
    "\n",
    "- info[‘instances’][i][‘bbox_label_3d’]: A int indicate the label of instance and the -1 indicate ignore.\n",
    "\n",
    "- info[‘instances’][i][‘velocity’]: Velocities of 3D bounding boxes (no vertical measurements due to inaccuracy), a list has shape (2.).\n",
    "\n",
    "- info[‘instances’][i][‘num_lidar_pts’]: Number of lidar points included in each 3D bounding box.\n",
    "\n",
    "- info[‘instances’][i][‘num_radar_pts’]: Number of radar points included in each 3D bounding box.\n",
    "\n",
    "- info[‘instances’][i][‘bbox_3d_isvalid’]: Whether each bounding box is valid. In general, we only take the 3D boxes that include at least one lidar or radar point as valid boxes.\n",
    "\n",
    "info[‘cam_instances’]: It is a dict containing keys 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_FRONT_LEFT', 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT'. For vision-based 3D object detection task, we split 3D annotations of the whole scenes according to the camera they belong to. For the i-th instance:\n",
    "\n",
    "- info[‘cam_instances’][‘CAM_XXX’][i][‘bbox_label’]: Label of instance.\n",
    "\n",
    "- info[‘cam_instances’][‘CAM_XXX’][i][‘bbox_label_3d’]: Label of instance.\n",
    "\n",
    "- info[‘cam_instances’][‘CAM_XXX’][i][‘bbox’]: 2D bounding box annotation (exterior rectangle of the projected 3D box), a list arrange as [x1, y1, x2, y2].\n",
    "\n",
    "- info[‘cam_instances’][‘CAM_XXX’][i][‘center_2d’]: Projected center location on the image, a list has shape (2,), .\n",
    "\n",
    "- info[‘cam_instances’][‘CAM_XXX’][i][‘depth’]: The depth of projected center.\n",
    "\n",
    "- info[‘cam_instances’][‘CAM_XXX’][i][‘velocity’]: Velocities of 3D bounding boxes (no vertical measurements due to inaccuracy), a list has shape (2,).\n",
    "\n",
    "- info[‘cam_instances’][‘CAM_XXX’][i][‘attr_label’]: The attr label of instance. We maintain a default attribute collection and mapping for attribute classification.\n",
    "\n",
    "- info[‘cam_instances’][‘CAM_XXX’][i][‘bbox_3d’]: List of 7 numbers representing the 3D bounding box of the instance, in (x, y, z, l, h, w, yaw) order.\n",
    "\n",
    "info[‘pts_semantic_mask_path’]：The filename of the lidar point cloud semantic segmentation annotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_cross_reference(root_dir, cross_file):\n",
    "    \"\"\"\n",
    "    Get the cross reference of patient_id between LIDC data and synthetic data\n",
    "    \"\"\"\n",
    "    cross_dict = dict()\n",
    "\n",
    "    cross_file = f'{root_dir}/{cross_file}'\n",
    "    with open(cross_file, 'r') as file: \n",
    "        for line in file: \n",
    "            lidc_ref, data_ref = line.strip().split(\",\")\n",
    "            cross_dict[data_ref.strip()] = lidc_ref\n",
    "    \n",
    "    return cross_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_images_path(root_dir, images_dir, patient_id):\n",
    "    \"\"\"\n",
    "    Get image path for each camera\n",
    "    \"\"\"\n",
    "\n",
    "    images_dir = f'{root_dir}/{images_dir}/Patient{patient_id:04}'\n",
    "\n",
    "    info = dict()\n",
    "\n",
    "    for cam in range(10):\n",
    "        cam_name = (f'CAM_{cam:02}:').upper()\n",
    "        info[cam_name] = f'{images_dir}/Image_{cam:02}.png'\n",
    "    \n",
    "    return info\n",
    "\n",
    "\n",
    "def get_3d_annotation(root_dir, anno3d_dir, patient_id):\n",
    "    \"\"\"\n",
    "    Get 3d annotation from raw .txt file\n",
    "    \"\"\"\n",
    "\n",
    "    anno_3d_file = f'{root_dir}/{anno3d_dir}/Patient_{patient_id:04}_bbox3d.txt'\n",
    "\n",
    "    instances = []\n",
    "    with open(anno_3d_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            instance_data = {}\n",
    "            # Convert the comma-separated values into floats\n",
    "            row = [float(value) for value in line.strip().split(\",\")]\n",
    "            row.append(0)  # add yaw value\n",
    "\n",
    "            instance_data['bbox_3d'] = row\n",
    "            instance_data['bbox_lavel_3d'] = 1\n",
    "            instance_data['bbox_3d_isvalid'] = True\n",
    "            instance_data['num_lidar_pts'] = 0\n",
    "            instance_data['num_rader_pts'] = 0\n",
    "            instance_data['velocity'] = [0.0, 0.0]\n",
    "\n",
    "            instances.append(instance_data)\n",
    "    return instances\n",
    "\n",
    "\n",
    "def get_2d_annotation(root_dir, anno2d_dir, patient_id):\n",
    "    \"\"\"\n",
    "    Get 2d annotation from raw .txt file\n",
    "    \"\"\"\n",
    "    anno_2d_dir = f'{root_dir}/{anno2d_dir}/Patient{patient_id:04}'\n",
    "\n",
    "    cam_instances = dict()\n",
    "\n",
    "    for cam in range(10):\n",
    "        bbox2d_file_path = f'{anno_2d_dir}/Cam_{cam:02}_bbox2d.txt'\n",
    "        cam_name = (f'CAM_{cam:02}:').upper()\n",
    "\n",
    "        with open(bbox2d_file_path, \"r\") as file:\n",
    "            cam_instance = []\n",
    "            for line in file:\n",
    "                instance_data = {}\n",
    "                row = [value for value in line.strip().split(\",\")]\n",
    "                instance_data['bbox'] = row[:4]\n",
    "                instance_data['bbox_label'] = 1\n",
    "                instance_data['bbox_label_3d'] = 1\n",
    "\n",
    "            cam_instance.append(instance_data)\n",
    "        cam_instances[cam_name] = cam_instance\n",
    "    return cam_instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "\n",
    "import os\n",
    "from os import path as osp\n",
    "import pickle\n",
    "\n",
    "# Arguments\n",
    "root_path = 'data/LIDC_Projection_Dataset'\n",
    "# Root directory to start walking\n",
    "# since we're in sandbox folder. \n",
    "root_dir = f'../{root_path}'\n",
    "\n",
    "info_prefix = 'lidc'\n",
    "version = 'v1.0'\n",
    "dataset_name = ' lidc'\n",
    "out_dir = root_dir\n",
    "images_dir = 'Images'\n",
    "anno_3d_dir = 'Labels3d'\n",
    "anno_2d_dir = 'Labels2d'\n",
    "\n",
    "\n",
    "db_info_save_path = osp.join(out_dir, f'{info_prefix}_dbinfos.pkl')\n",
    "info_train_path = osp.join(out_dir, f'{info_prefix}_dbinfos_train.pkl')\n",
    "info_val_path = osp.join(out_dir, f'{info_prefix}_dbinfos_val.pkl')\n",
    "error_log_path = osp.join(out_dir, f'{info_prefix}_error_logs.txt')\n",
    "\n",
    "# Get the cross reference\n",
    "cross_file = 'patients_processed.txt'\n",
    "cross_ref = get_id_cross_reference(root_dir, cross_file)\n",
    "nb_patient = len(cross_ref)\n",
    "\n",
    "\n",
    "# Initalize list\n",
    "all_db_infos = dict()  # Store all data info\n",
    "logs = dict()  # Store error logs (if any)\n",
    "\n",
    "\n",
    "# Build metainfo of the dataset\n",
    "metainfo = {\n",
    "    'categories': {'normal': 0,'nodule': 1},\n",
    "    'dataset': 'lidc',\n",
    "    'version': 'v1.0',\n",
    "    'info_version': '1.0'\n",
    "}\n",
    "all_db_infos['metainfo'] = metainfo\n",
    "\n",
    "\n",
    "# Build the ground truth database\n",
    "all_db_datalist = []  # Store all datalist (inside db_infos)\n",
    "\n",
    "for i in range(nb_patient):\n",
    "\n",
    "    info_data = dict()\n",
    "\n",
    "    try: \n",
    "        # Build patient_id meta data \n",
    "        info_data['sample_id'] = i\n",
    "        info_data['lidc_id_ref'] = cross_ref[str(i)]\n",
    "\n",
    "        # Build Images paths\n",
    "        info_data['images'] = get_images_path(root_dir, images_dir=images_dir, patient_id=i)\n",
    "\n",
    "        # Build 3D annotation paths\n",
    "        info_data['instances'] = get_3d_annotation(root_dir, anno_3d_dir, i)\n",
    "\n",
    "        # Build 2D annotation paths\n",
    "        info_data['cam_instances'] = get_2d_annotation(root_dir, anno_2d_dir, i)\n",
    "\n",
    "        # Write to datalist\n",
    "        all_db_datalist.append(info_data)\n",
    "\n",
    "    except Exception as e: \n",
    "        logs[f'Patient_{i:04}'] = str(e)\n",
    "        continue\n",
    "\n",
    "all_db_infos['data_list'] = all_db_datalist\n",
    "\n",
    "# Write to disk\n",
    "with open(db_info_save_path, 'wb') as f:\n",
    "    pickle.dump(all_db_infos, f)\n",
    "\n",
    "# Write error log file\n",
    "with open(error_log_path, 'w') as f:\n",
    "    for key, value in logs.items():\n",
    "        f.write(f'{key}, {value}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data infos all\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(db_info_save_path, 'rb') as f:\n",
    "    db_all_infos = pickle.load(f)\n",
    "\n",
    "len(db_all_infos['data_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "import numpy as np\n",
    "\n",
    "data = db_all_infos['data_list']\n",
    "bbox_test = data[0]['cam_instances']['CAM_00:'][0]['bbox']  # store as a list\n",
    "bbox_test = np.array([[0, 0, 50, 50], [20, 20, 60, 60]])\n",
    "img_test = data[0]['images']['CAM_00:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0, 255,   0],\n",
       "        [  0, 255,   0],\n",
       "        [  0, 255,   0],\n",
       "        ...,\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127]],\n",
       "\n",
       "       [[  0, 255,   0],\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127],\n",
       "        ...,\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127]],\n",
       "\n",
       "       [[  0, 255,   0],\n",
       "        [126, 126, 126],\n",
       "        [126, 126, 126],\n",
       "        ...,\n",
       "        [127, 127, 127],\n",
       "        [126, 126, 126],\n",
       "        [126, 126, 126]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[128, 128, 128],\n",
       "        [128, 128, 128],\n",
       "        [128, 128, 128],\n",
       "        ...,\n",
       "        [129, 129, 129],\n",
       "        [128, 128, 128],\n",
       "        [128, 128, 128]],\n",
       "\n",
       "       [[129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        ...,\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129]],\n",
       "\n",
       "       [[129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        ...,\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129]]], dtype=uint8)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mmcv.imshow(img_test)\n",
    "mmcv.imshow_bboxes(img_test, bbox_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the folder\n",
    "folder_path = f'{root_dir}/Labels3d'\n",
    "\n",
    "# find all Patient that has bbox_3d\n",
    "txt_files = [file for file in os.listdir(folder_path) if file.endswith(\".txt\")]\n",
    "txt_files.sort()\n",
    "patient_idx = [int(file.split('_')[1]) for file in txt_files]\n",
    "\n",
    "# find patient that does not have bbox3d\n",
    "patient_no_bbox3d = list(set(range(911)) - set(patient_idx))\n",
    "patient_no_bbox3d.sort()\n",
    "\n",
    "for i in patient_idx:\n",
    "\n",
    "    info = dict()\n",
    "    info['sample_idx'] = i\n",
    "\n",
    "    images_dir = f'{root_dir}/Images/Patient{i:04}'\n",
    "    anno_2d_dir = f'{root_dir}/Labels2d/Patient{i:04}'\n",
    "    anno_3d_file = f'{root_dir}/Labels3d/Patient_{i:04}_bbox3d.txt'\n",
    "\n",
    "    info['images'] = get_images_path(images_dir)\n",
    "    info['instances'] = get_3d_annotation(anno_3d_file)\n",
    "    info['cam_instances'] = get_2d_annotation(anno_2d_dir)\n",
    "    \n",
    "    data_list.append(info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MV2Denv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
