{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data list info\n",
    "\n",
    "We follow the format of `nuscenes` dataset\n",
    "\n",
    "### Helper functions and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from os import path as osp\n",
    "import pickle\n",
    "import numpy as np\n",
    "import mmcv\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/ensai/Documents/msd06-1-smart-data-project/LIDC/MV2Dbasedlifting\n"
     ]
    }
   ],
   "source": [
    "# Set the working directory to the project folder\n",
    "os.chdir('../')\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Set a constant timestamp for all image\n",
    "# \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def date_to_microsecond_timestamp(date_str):\n",
    "    \"\"\"\n",
    "    Convert a date string in 'YYYY-MM-DD' format to a Unix timestamp in microseconds.\n",
    "\n",
    "    Args:\n",
    "        date_str (str): Date string in the format 'YYYY-MM-DD'.\n",
    "\n",
    "    Returns:\n",
    "        int: Unix timestamp in microseconds.\n",
    "    \"\"\"\n",
    "    # Parse the date string to a datetime object\n",
    "    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Convert to Unix timestamp in seconds\n",
    "    timestamp_seconds = int(date_obj.timestamp())\n",
    "    \n",
    "    # Convert to microseconds\n",
    "    timestamp_microseconds = timestamp_seconds * 1_000_000\n",
    "    return timestamp_microseconds\n",
    "\n",
    "\n",
    "# Set the timestamp to a certain date to put into anno file\n",
    "date_str = \"2025-01-01\"\n",
    "const_timestamp = date_to_microsecond_timestamp(date_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_id_cross_reference(root_dir, cross_file):\n",
    "    \"\"\"\n",
    "    Get the cross reference of patient_id between LIDC data and synthetic data\n",
    "    \"\"\"\n",
    "    cross_dict = dict()\n",
    "\n",
    "    cross_file = f\"{root_dir}/{cross_file}\"\n",
    "    with open(cross_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            lidc_ref, data_ref = line.strip().split(\",\")\n",
    "            cross_dict[data_ref.strip()] = lidc_ref\n",
    "\n",
    "    return cross_dict\n",
    "\n",
    "\n",
    "def get_cam_data(root_dir, images_dir, patient_id):\n",
    "    \"\"\"\n",
    "    Get image path for each camera\n",
    "    \"\"\"\n",
    "\n",
    "    images_dir = f\"{root_dir}/{images_dir}/Patient{patient_id:04}\"\n",
    "\n",
    "    info = dict()\n",
    "\n",
    "    for cam in range(10):\n",
    "        info_cam = dict()\n",
    "\n",
    "        cam_name = (f\"CAM_{cam:02}\").upper()\n",
    "        info_cam[\"data_path\"] = f\"{images_dir}/Image_{cam:02}.png\"\n",
    "        info_cam[\"type\"] = cam_name\n",
    "        info_cam[\"sample_data_token\"] = f\"{patient_id:03}cam{cam:02}\"\n",
    "        info_cam[\"timestamp\"] = const_timestamp\n",
    "\n",
    "        # Get cam intrinsic value\n",
    "        cam_intrinsic_file = (\n",
    "            f\"{root_dir}/Cams/Patient{patient_id:04}/{patient_id:04}_{cam:02}.txt\"\n",
    "        )\n",
    "        with open(cam_intrinsic_file, \"r\") as file:\n",
    "            for line in file:\n",
    "                cam_intrinsic = [float(value) for value in line.strip().split(\",\")[:9]]\n",
    "        info_cam[\"cam_intrinsic\"] = np.array(cam_intrinsic).reshape((3, 3))\n",
    "\n",
    "        info[cam_name] = info_cam\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "def get_3d_annotation(root_dir, anno3d_dir, patient_id):\n",
    "    \"\"\"\n",
    "    Get 3d annotation from raw .txt file\n",
    "    Return gt_boxed and gt_label\n",
    "    For LIDC we only have 1 label\n",
    "    \"\"\"\n",
    "\n",
    "    anno_3d_file = f\"{root_dir}/{anno3d_dir}/Patient_{patient_id:04}_bbox3d.txt\"\n",
    "\n",
    "    gt_boxes = []\n",
    "    with open(anno_3d_file, \"r\") as file:\n",
    "        for line in file:\n",
    "\n",
    "            # Convert the comma-separated values into floats\n",
    "            # the coordinates in the txt file were stored as Y, X, Z (as well as the extension)\n",
    "            # we need to convert it to X, Y, Z, dX, dY, dZ\n",
    "            row = [float(value) for value in line.strip().split(\",\")]\n",
    "            row = np.array(row)[[1, 0, 2, 4, 3, 5]]\n",
    "            row = np.append(row, 0)  # add yaw value\n",
    "\n",
    "            gt_boxes.append(row)\n",
    "\n",
    "    gt_boxes = np.array(gt_boxes)\n",
    "    gt_names = np.array([\"nodule\"] * len(gt_boxes), dtype=\"<U32\")\n",
    "\n",
    "    return gt_boxes, gt_names\n",
    "\n",
    "\n",
    "# def get_2d_annotation(root_dir, anno2d_dir, patient_id):\n",
    "#     \"\"\"\n",
    "#     Get 2d annotation from raw .txt file\n",
    "#     \"\"\"\n",
    "#     anno_2d_dir = f'{root_dir}/{anno2d_dir}/Patient{patient_id:04}'\n",
    "\n",
    "#     cam_instances = dict()\n",
    "\n",
    "#     for cam in range(10):\n",
    "#         bbox2d_file_path = f'{anno_2d_dir}/Cam_{cam:02}_bbox2d.txt'\n",
    "#         cam_name = (f'CAM_{cam:02}:').upper()\n",
    "\n",
    "#         with open(bbox2d_file_path, \"r\") as file:\n",
    "#             cam_instance = []\n",
    "#             for line in file:\n",
    "#                 instance_data = {}\n",
    "#                 row = [value for value in line.strip().split(\",\")]\n",
    "#                 instance_data['bbox'] = row[:4]\n",
    "#                 instance_data['bbox_label'] = 1\n",
    "#                 instance_data['bbox_label_3d'] = 1\n",
    "\n",
    "#             cam_instance.append(instance_data)\n",
    "#         cam_instances[cam_name] = cam_instance\n",
    "#     return cam_instances\n",
    "\n",
    "\n",
    "def get_2d_annotation(root_dir, images_dir, anno_3d_dir, anno_2d_files):\n",
    "    \"\"\"Build 2D annotation data\"\"\"\n",
    "\n",
    "    annotations = []\n",
    "    images = []\n",
    "    anno_id = 0\n",
    "\n",
    "    infos_2d_anno = dict()\n",
    "\n",
    "    for path in anno_2d_files:\n",
    "\n",
    "        image = dict()\n",
    "\n",
    "        # Get info about image\n",
    "        # multiple annotations for 1 image are recorded seperately\n",
    "        patient_id = path.parts[-2][-4:]\n",
    "        cam_id = path.parts[-1].split(\"_\")[1]\n",
    "        # TODO: check if image file exist\n",
    "        # file_name excudes the image path: data/lidc/Image/\n",
    "        file_name = f\"Patient{patient_id}/Image_{cam_id}.png\"\n",
    "        image[\"file_name\"] = file_name\n",
    "        image[\"id\"] = f\"{patient_id}cam{cam_id}\"\n",
    "\n",
    "        # Get info about cam intrinsic\n",
    "        cam_intrinsics = get_cam_data(root_dir, images_dir, int(patient_id))\n",
    "        image[\"cam_intrinsic\"] = cam_intrinsics[f\"CAM_{cam_id}\"]\n",
    "        image[\"width\"] = 1024\n",
    "        image[\"height\"] = 1024\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "        # Get info about bbox 3D\n",
    "        # TODO: check if the nb_bbox_2d = nb_bbox_3d\n",
    "        bbox_3ds, _ = get_3d_annotation(root_dir, anno_3d_dir, int(patient_id))\n",
    "\n",
    "        # Get info about bbox\n",
    "        with open(path, \"r\") as file:\n",
    "            for idx, line in enumerate(file):\n",
    "                anno2d = dict()\n",
    "\n",
    "                row = [value for value in line.strip().split(\",\")]\n",
    "                x, y, dx, dy = [float(x) for x in row[:4]]\n",
    "                category_name = row[4]\n",
    "\n",
    "                anno2d[\"file_name\"] = file_name\n",
    "                anno2d[\"image_id\"] = f\"{patient_id}cam{cam_id}\"\n",
    "                anno2d[\"area\"] = dx * dy\n",
    "                anno2d[\"category_name\"] = category_name\n",
    "                anno2d[\"category_id\"] = 1\n",
    "                anno2d[\"bbox\"] = [x, y, dx, dy]\n",
    "                anno2d[\"iscrowd\"] = 0\n",
    "                anno2d[\"bbox_cam3d\"] = bbox_3ds[idx]\n",
    "                # TODO: check if we need center2d info - list of 3\n",
    "                anno2d[\"center2d\"] = [0, 0, 0]\n",
    "                anno2d[\"id\"] = anno_id\n",
    "\n",
    "                # Additional information\n",
    "                # subtlety, internalStructure, calcification, sphericity, margin, lobulation, spiculation, texture, malignancy\n",
    "                add_info = [\n",
    "                    \"subtlety\",\n",
    "                    \"internalStructure\",\n",
    "                    \"calcification\",\n",
    "                    \"sphericity\",\n",
    "                    \"margin\",\n",
    "                    \"lobulation\",\n",
    "                    \"spiculation\",\n",
    "                    \"texture\",\n",
    "                    \"malignancy\",\n",
    "                ]\n",
    "                for i, info_type in enumerate(add_info, start=5):\n",
    "                    anno2d[info_type] = row[i]\n",
    "\n",
    "                # Write info to list\n",
    "                annotations.append(anno2d)\n",
    "                anno_id += 1\n",
    "\n",
    "    infos_2d_anno[\"annotations\"] = annotations\n",
    "    infos_2d_anno[\"images\"] = images\n",
    "\n",
    "    return infos_2d_anno\n",
    "\n",
    "\n",
    "def train_test_split(data_infos, metadata, train_split=0.8):\n",
    "    \"\"\"Split data into train_set and test_set\"\"\"\n",
    "\n",
    "    infos_train = dict()\n",
    "    infos_val = dict()\n",
    "\n",
    "    # Shuffle the items randomly\n",
    "    random.shuffle(data_infos)\n",
    "\n",
    "    # Split data\n",
    "    train_end = int(train_split * len(data_infos))\n",
    "\n",
    "    train_items = data_infos[:train_end]\n",
    "    val_items = data_infos[train_end:]\n",
    "\n",
    "    # train_set = dict(train_items)\n",
    "    # val_set = dict(val_items)\n",
    "    train_perc = len(train_items) / len(data_infos) * 100\n",
    "    val_perc = len(val_items) / len(data_infos) * 100\n",
    "\n",
    "    print(f\"Length train_set: {len(train_items)} [{train_perc:.2f}%]\")\n",
    "    print(f\"Length val_set: {len(val_items)} [{val_perc:.2f}%]\")\n",
    "\n",
    "    # Update infos dict\n",
    "    metadata_train = metadata.copy()\n",
    "    metadata_val = metadata.copy()\n",
    "    metadata_train.update({\"validation_set\": False})\n",
    "    metadata_val.update({\"validation_set\": True})\n",
    "\n",
    "    infos_train.update({\"metadata\": metadata_train, \"infos\": train_items})\n",
    "    infos_val.update({\"metadata\": metadata_val, \"infos\": val_items})\n",
    "\n",
    "    return infos_train, infos_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "\n",
    "# Arguments\n",
    "root_path = \"data/lidc\"\n",
    "# Root directory to start walking\n",
    "root_dir = f\"./{root_path}\"\n",
    "\n",
    "info_prefix = \"lidc\"\n",
    "version = \"v1.0\"\n",
    "dataset_name = \" lidc\"\n",
    "out_dir = root_dir\n",
    "images_dir = \"Images\"\n",
    "anno_3d_dir = \"Labels3d\"\n",
    "anno_2d_dir = \"Labels2d\"\n",
    "\n",
    "\n",
    "db_info_save_path = osp.join(out_dir, f\"{info_prefix}_dbinfos.pkl\")\n",
    "\n",
    "# Train data save path\n",
    "info_train_path = osp.join(out_dir, f\"{info_prefix}_infos_train.pkl\")\n",
    "info_train_2d_anno_path = osp.join(\n",
    "    out_dir, f\"{info_prefix}_infos_train_2d_anno.coco.json\"\n",
    ")\n",
    "\n",
    "# Val data save path\n",
    "info_val_path = osp.join(out_dir, f\"{info_prefix}_infos_val.pkl\")\n",
    "info_val_2d_anno_path = osp.join(\n",
    "    out_dir, f\"{info_prefix}_infos_val_2d_anno.coco.json\"\n",
    ")\n",
    "\n",
    "\n",
    "error_log_path = osp.join(out_dir, f\"{info_prefix}_error_logs.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate `infos_train_pkl` for bbox3D annotations and image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish creating infos database\n",
      "Length train_set: 632 [79.90%]\n",
      "Length val_set: 159 [20.10%]\n",
      "Write train set info into ./data/lidc/lidc_infos_train.pkl\n",
      "Write validation set info into ./data/lidc/lidc_infos_val.pkl\n",
      "Write error log info into ./data/lidc/lidc_error_logs.txt\n"
     ]
    }
   ],
   "source": [
    "# Get the cross reference\n",
    "cross_file = \"patients_processed.txt\"\n",
    "cross_ref = get_id_cross_reference(root_dir, cross_file)\n",
    "nb_patient = len(cross_ref)\n",
    "\n",
    "\n",
    "# Initalize list\n",
    "lidc_infos_train = dict()  # Store all data info\n",
    "logs = dict()  # Store error logs (if any)\n",
    "\n",
    "\n",
    "# Build metainfo of the dataset\n",
    "metadata = {\n",
    "    \"categories\": {\"normal\": 0, \"nodule\": 1},\n",
    "    \"dataset\": \"lidc\",\n",
    "    \"version\": \"v1.0\",\n",
    "    \"info_version\": \"1.0\",\n",
    "}\n",
    "# lidc_infos_train['metadata'] = metadata\n",
    "\n",
    "\n",
    "# Build the ground truth 3D database\n",
    "infos = []  # Store all datalist (inside db_infos)\n",
    "\n",
    "\n",
    "for i in range(nb_patient):\n",
    "\n",
    "    info_data = dict()\n",
    "\n",
    "    info_data[\"token\"] = i\n",
    "\n",
    "    try:\n",
    "        # Build patient_id meta data\n",
    "        info_data[\"sample_id\"] = i\n",
    "        info_data[\"lidc_id_ref\"] = cross_ref[str(i)]\n",
    "\n",
    "        # Build cams data\n",
    "        info_data[\"cams\"] = get_cam_data(root_dir, images_dir=images_dir, patient_id=i)\n",
    "\n",
    "        # Build 3D annotation data\n",
    "        info_data[\"gt_boxes\"], info_data[\"gt_names\"] = get_3d_annotation(\n",
    "            root_dir, anno_3d_dir, i\n",
    "        )\n",
    "\n",
    "        # Set valid flag = True b.c we dont have any lidars_ptd or radars_pts\n",
    "        info_data[\"valid_flag\"] = np.array([True] * len(info_data[\"gt_boxes\"]))\n",
    "\n",
    "        # Build 2D annotation paths\n",
    "        # info_data['cam_instances'] = get_2d_annotation(root_dir, anno_2d_dir, i)\n",
    "\n",
    "        # Write to datalist\n",
    "        infos.append(info_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        logs[f\"Patient_{i:04}\"] = str(e)\n",
    "        continue\n",
    "\n",
    "print(\"Finish creating infos database\")\n",
    "# lidc_infos_train['infos'] = infos\n",
    "\n",
    "\n",
    "# Split to Train - Validation dataset.\n",
    "lidc_infos_train, lidc_infos_val = train_test_split(\n",
    "    data_infos=infos, metadata=metadata, train_split=0.8\n",
    ")\n",
    "\n",
    "# Write to disk\n",
    "with open(info_train_path, \"wb\") as f:\n",
    "    pickle.dump(lidc_infos_train, f)\n",
    "    print(f\"Write train set info into {info_train_path}\")\n",
    "\n",
    "with open(info_val_path, \"wb\") as f:\n",
    "    pickle.dump(lidc_infos_val, f)\n",
    "    print(f\"Write validation set info into {info_val_path}\")\n",
    "\n",
    "# Write error log file\n",
    "with open(error_log_path, \"w\") as f:\n",
    "    for key, value in logs.items():\n",
    "        f.write(f\"{key}, {value}\\n\")\n",
    "    print(f\"Write error log info into {error_log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate `infos_train_2d_anno` for 2D Annotation file with COCO `.json` format\n",
    "\n",
    "We will store the anno_2d file in COCO format to match with current MV2D data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get annotation 2D *.txt files for train patients [6320 files]\n",
      "Get annotation 2D *.txt files for validation patients [1588 files]\n",
      "Finish building 2d annotation for train dataset\n",
      "Finish building 2d annotation for val dataset\n",
      "Write 2d annotation for train at ./data/lidc/lidc_infos_train_2d_anno.coco.json\n",
      "Write 2d annotation for validation at ./data/lidc/lidc_infos_val_2d_anno.coco.json\n"
     ]
    }
   ],
   "source": [
    "# Function to get annotation files for a set of patients\n",
    "def get_annotation_files(anno_2d_path, patient_list):\n",
    "    # Get 2D annotation paths for patients\n",
    "    anno_2d_path = [\n",
    "        folder\n",
    "        for folder in anno_2d_path.iterdir()\n",
    "        if folder.is_dir() and folder.name in patient_list\n",
    "    ]\n",
    "    anno_2d_files = [\n",
    "        file for path in anno_2d_path for file in path.rglob(\"*.txt\") if file.is_file()\n",
    "    ]\n",
    "    return sorted(anno_2d_files)\n",
    "\n",
    "\n",
    "# 2D annotation path\n",
    "anno_2d_dir = f\"{root_dir}/Labels2d\"\n",
    "anno_2d_path = Path(anno_2d_dir)\n",
    "\n",
    "# Get idx of train_patient and val_patient\n",
    "train_patient = [f\"Patient{i['sample_id']:04}\" for i in lidc_infos_train[\"infos\"]]\n",
    "val_patient = [f\"Patient{i['sample_id']:04}\" for i in lidc_infos_val[\"infos\"]]\n",
    "\n",
    "# Get annotation 2D *.txt files for train and validation patients\n",
    "train_anno_2d_files = get_annotation_files(anno_2d_path, train_patient)\n",
    "print(\n",
    "    f\"Get annotation 2D *.txt files for train patients [{len(train_anno_2d_files)} files]\"\n",
    ")\n",
    "\n",
    "val_anno_2d_files = get_annotation_files(anno_2d_path, val_patient)\n",
    "print(\n",
    "    f\"Get annotation 2D *.txt files for validation patients [{len(val_anno_2d_files)} files]\"\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize data dict\n",
    "lidc_infos_train_2d_anno = dict()\n",
    "lidc_infos_val_2d_anno = dict()\n",
    "\n",
    "\n",
    "# Create label data\n",
    "categories_data = [{\"id\": 0, \"name\": \"normal\"}, {\"id\": 1, \"name\": \"nodule\"}]\n",
    "\n",
    "\n",
    "# Build 2D annotation data\n",
    "lidc_infos_train_2d_anno = get_2d_annotation(\n",
    "    root_dir=root_dir,\n",
    "    images_dir=images_dir,\n",
    "    anno_3d_dir=anno_3d_dir,\n",
    "    anno_2d_files=train_anno_2d_files,\n",
    ")\n",
    "lidc_infos_train_2d_anno[\"categories\"] = categories_data\n",
    "print(\"Finish building 2d annotation for train dataset\")\n",
    "\n",
    "lidc_infos_val_2d_anno = get_2d_annotation(\n",
    "    root_dir=root_dir,\n",
    "    images_dir=images_dir,\n",
    "    anno_3d_dir=anno_3d_dir,\n",
    "    anno_2d_files=val_anno_2d_files,\n",
    ")\n",
    "lidc_infos_val_2d_anno[\"categories\"] = categories_data\n",
    "print(\"Finish building 2d annotation for val dataset\")\n",
    "\n",
    "\n",
    "# Write to disk\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"Handle n.array when writing to JSON\"\"\"\n",
    "\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert ndarray to list\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "with open(info_train_2d_anno_path, \"w\") as f:\n",
    "    # pickle.dump(lidc_infos_train_2d_anno, f)\n",
    "    json.dump(lidc_infos_train_2d_anno, f, cls=NumpyEncoder, indent=4)\n",
    "    print(f\"Write 2d annotation for train at {info_train_2d_anno_path}\")\n",
    "\n",
    "\n",
    "with open(info_val_2d_anno_path, \"w\") as f:\n",
    "    # pickle.dump(lidc_infos_train_2d_anno, f)\n",
    "    json.dump(lidc_infos_val_2d_anno, f, cls=NumpyEncoder, indent=4)\n",
    "    print(f\"Write 2d annotation for validation at {info_val_2d_anno_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(info_train_path, \"rb\") as f:\n",
    "    lidc_infos_train = pickle.load(f)\n",
    "\n",
    "len(lidc_infos_train[\"infos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49149"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(info_train_2d_anno_path, \"r\") as f:\n",
    "    lidc_infos_train_2d_anno = json.load(f)\n",
    "\n",
    "len(lidc_infos_train_2d_anno[\"annotations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check bbox information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[369.0, 567.0, 54.0, 70.0],\n",
       " [371.0, 569.0, 46.0, 72.0],\n",
       " [371.0, 569.0, 52.0, 66.0],\n",
       " [363.0, 569.0, 60.0, 66.0]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_2d_all = [\n",
    "    d\n",
    "    for d in lidc_infos_train_2d_anno[\"annotations\"]\n",
    "    if \"Patient0000/Image_00.png\" in d[\"file_name\"]\n",
    "]\n",
    "len(anno_2d_all)\n",
    "p00_bboxes = [d[\"bbox\"] for d in anno_2d_all]\n",
    "p00_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[369., 567., 423., 497.],\n",
       "       [371., 569., 417., 497.],\n",
       "       [371., 569., 423., 503.],\n",
       "       [363., 569., 423., 503.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p00_bboxes_convert = np.array(\n",
    "    [[d[0], d[1], d[0] + d[2], d[1] - d[3]] for d in p00_bboxes]\n",
    ")\n",
    "p00_bboxes_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[127, 127, 127],\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127],\n",
       "        ...,\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127]],\n",
       "\n",
       "       [[127, 127, 127],\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127],\n",
       "        ...,\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127],\n",
       "        [127, 127, 127]],\n",
       "\n",
       "       [[126, 126, 126],\n",
       "        [126, 126, 126],\n",
       "        [126, 126, 126],\n",
       "        ...,\n",
       "        [127, 127, 127],\n",
       "        [126, 126, 126],\n",
       "        [126, 126, 126]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[128, 128, 128],\n",
       "        [128, 128, 128],\n",
       "        [128, 128, 128],\n",
       "        ...,\n",
       "        [129, 129, 129],\n",
       "        [128, 128, 128],\n",
       "        [128, 128, 128]],\n",
       "\n",
       "       [[129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        ...,\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129]],\n",
       "\n",
       "       [[129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        ...,\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129],\n",
       "        [129, 129, 129]]], dtype=uint8)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mmcv.imshow(test_img)\n",
    "test_img = test_anno2d[\"file_name\"]\n",
    "x, y, dx, dy = test_anno2d[\"bbox\"]\n",
    "test_bbox = np.array([x, y, x + dx, y - dy]).reshape((1, 4))\n",
    "img = Image.open(test_img)\n",
    "width, height = img.size\n",
    "# width, height\n",
    "# img\n",
    "\n",
    "# mmcv.imshow(test_img)\n",
    "mmcv.imshow_bboxes(test_img, p00_bboxes_convert, colors=\"red\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MV2Denv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
